import requests
from bs4 import BeautifulSoup
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor
from selenium.webdriver.common.keys import Keys

from time import sleep
from tqdm import tqdm

import re
# è¨­ç½® headersï¼Œæ¨¡æ“¬ç€è¦½å™¨è¡Œç‚ºï¼Œé˜²æ­¢è«‹æ±‚è¢«æ‹’çµ•
class Crawl:
    my_headers={
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36"
    }       
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--enable-unsafe-swiftshader")          
 
    @classmethod
    def carrefour(cls):
        # å®¶æ¨‚ç¦çš„é¦–é  URL
        url = 'https://online.carrefour.com.tw/zh/homepage/'
   
        def get_soup(url):
            """ç™¼é€ GET è«‹æ±‚ä¸¦è¿”å›è§£æå¾Œçš„ BeautifulSoup å°è±¡"""
            try:
                response = requests.get(url, headers=cls.my_headers, timeout=10)
                if response.status_code == 200:
                    return BeautifulSoup(response.text, 'html.parser')
            except requests.RequestException as e:
                print(f"è«‹æ±‚å¤±æ•—: {url}, éŒ¯èª¤: {e}")
            return None

        def get_category_links():
            """ç²å–æ‰€æœ‰å•†å“åˆ†é¡çš„éˆæ¥"""
            soup = get_soup(url)
            if soup:
                # æå–æ‰€æœ‰åˆ†é¡çš„éˆæ¥
                category_links = ['https://online.carrefour.com.tw' + a.get('href') for a in soup.find_all('a', class_='category-panel-label')]
                return category_links[9:]
                # return category_links[9:10]
            return []

        def scrape_category_page(category_url):
            """æŠ“å–æŸä¸€åˆ†é¡é é¢çš„æ‰€æœ‰å•†å“"""
            all_products = []
            page = 0
            while True:
                paginated_url = f"{category_url}?start={page * 24}"
                soup = get_soup(paginated_url)
                if not soup:
                    break

                # æå–åˆ†é¡åç¨±
                try:
                    category = soup.select('div.crumbs a')[-1].text.split('/')[1].strip()
                except (IndexError, AttributeError):
                    category = "æœªçŸ¥åˆ†é¡"

                # æå–å•†å“ä¿¡æ¯
                products = soup.select('div.desc-operation-wrapper a')
                prices = soup.find_all('div', class_='current-price')
                img_urls = soup.select('div.box-img img')

                for product, price, img_url in zip(products, prices, img_urls):
                    title = product.text
                    product_url = product.get('href')
                    price = price.text.split('$')[1].strip()
                    img_url = img_url.get('src')[31:]

                    all_products.append({
                        'name': title,
                        'price': price,
                        'img_url': img_url,
                        'product_url': product_url,
                        'category': category,
                        'store': 'carrefour'
                    })

                    print(title, product_url, price, category)
                
                # å¦‚æœè©²é é¢æ²’æœ‰æ›´å¤šå•†å“å‰‡åœæ­¢
                if len(products) < 24:
                    break

                page += 1

            return all_products
        
        """æŠ“å–æ‰€æœ‰åˆ†é¡çš„å•†å“ï¼Œä½¿ç”¨ä½µç™¼ä¾†åŠ é€Ÿ"""
        category_urls = get_category_links()
        all_products = []

        # ä½¿ç”¨ ThreadPoolExecutor ä¾†ä¸¦è¡ŒæŠ“å–å„å€‹åˆ†é¡é é¢
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = executor.map(scrape_category_page, category_urls)
            for result in results:
                all_products.extend(result)

        return all_products
    @classmethod
    def costco(cls):  
        # è¨­å®šå¥½äº‹å¤šç·šä¸Šå•†åŸçš„é¦–é  URL
        url = 'https://www.costco.com.tw/'

        def get_soup(url):
            """ç™¼é€ GET è«‹æ±‚ä¸¦å›å‚³ BeautifulSoup è§£æå¾Œçš„å…§å®¹"""
            try:
                response = requests.get(url, headers=cls.my_headers, timeout=10)
                if response.status_code == 200:
                    return BeautifulSoup(response.text, 'html.parser')
            except requests.RequestException as e:
                print(f"è«‹æ±‚å¤±æ•—: {url}, éŒ¯èª¤: {e}")
            return None

        def get_category_links():
            """æŠ“å–æ‰€æœ‰å•†å“åˆ†é¡çš„éˆæ¥"""
            soup = get_soup(url)
            if soup:
                link_list = [a.get('href') for a in soup.select('ul#theMenu > li > ul > li > a.ng-star-inserted')]
                link_list = list(filter(lambda x: x != 'javascript:void(0)' and x != 'https://www.costco.com.tw/Televisions-Appliances/c/395', link_list))  # éæ¿¾æ‰ç„¡æ•ˆéˆæ¥
                return link_list[20:88]
                # return link_list[25:27]
            return []  # è‹¥è«‹æ±‚å¤±æ•—æˆ–ç„¡åˆ†é¡å‰‡è¿”å›ç©ºåˆ—è¡¨

        def scrape_category_page(category_url):
            """æŠ“å–æŸä¸€åˆ†é¡é é¢çš„æ‰€æœ‰å•†å“"""
            all_products = []
            page = 0
            while True:
                paginated_url = category_url if page == 0 else f"{category_url}?page={page}"
                print(paginated_url)
                soup = get_soup(paginated_url)
                if not soup:
                    break
                
                # æå–åˆ†é¡åç¨±
                try:
                    category = soup.select('div.breadcrumb-section a')[-2].text.strip()
                except (IndexError, AttributeError):
                    category = "æœªçŸ¥åˆ†é¡"    
                
                # æå–å•†å“ä¿¡æ¯
                products = soup.find_all('a',class_='lister-name js-lister-name')
                prices = soup.select('div.product-price span.notranslate')
                img_urls = soup.select('div.product-image > a > sip-primary-image > sip-media > picture > img')
                
                for product, price, img_url in zip(products, prices, img_urls):
                    title = product.text
                    product_url = product.get('href').replace('https://www.costco.com.tw', '')
                    # try:
                    price = price.text.split('$')[1].strip()
                    # except IndexError:
                    #     price = "æœªçŸ¥åƒ¹æ ¼"
                    img_url = img_url.get('src').replace('https://www.costco.com.tw', '')
                
                    all_products.append({
                        'name': title,
                        'price': price,
                        'img_url': img_url,
                        'product_url': product_url,
                        'category': category,
                        'store': 'costco'
                    })
                
                    print(title, product_url, price, category)
                    
                    # å¦‚æœè©²é é¢æ²’æœ‰æ›´å¤šå•†å“å‰‡åœæ­¢
                if len(products) < 48:
                    break
                
                page += 1
            return all_products


        """æŠ“å–æ‰€æœ‰åˆ†é¡çš„å•†å“ï¼Œä½¿ç”¨ä½µç™¼ä¾†åŠ é€Ÿ"""
        category_urls = get_category_links()
        all_products = []

        # ä½¿ç”¨ ThreadPoolExecutor ä¾†ä¸¦è¡ŒæŠ“å–å„å€‹åˆ†é¡é é¢
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = executor.map(scrape_category_page, category_urls)
            for result in results:
                all_products.extend(result)

        return all_products
    @classmethod
    def savesafe(cls):
        
        # === çˆ¬å–å•†å“è©³ç´°è³‡è¨Š ===
        def get_product_details(driver, product_url):
            """
            å¾æŒ‡å®šçš„å•†å“é é¢æŠ“å–å•†å“è©³ç´°è³‡è¨Šï¼ŒåŒ…æ‹¬æ¨™é¡Œã€åƒ¹æ ¼ã€åœ–ç‰‡åŠåˆ†é¡ï¼Œ
            ç„¶å¾Œå°‡é€™äº›è³‡è¨Šå­˜å…¥è³‡æ–™åº«ã€‚

            :param driver: Selenium çš„ WebDriver ç‰©ä»¶ï¼Œç”¨ä¾†æ“ä½œç€è¦½å™¨
            :param url: å•†å“çš„è©³ç´°é é¢ç¶²å€
            """
            driver.get(product_url)
            try:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h4.h2-responsive.product-name')))

                # âœ… æŠ“å–æ¨™é¡Œ
                try:
                    title_element = driver.find_element(By.CSS_SELECTOR, 'h4.h2-responsive.product-name')
                    title = title_element.text.strip()
                    title = re.sub(r'\s*<span>.*?</span>\s*', '', title)  # ç§»é™¤ <span> å…§çš„å…§å®¹
                except:
                    title = None

                # âœ… æŠ“å–åƒ¹æ ¼
                try:
                    price_element = driver.find_element(By.CSS_SELECTOR, 'span.SalePrice.text-danger')
                    price = re.sub(r"[^\d.]", "", price_element.text.strip())  # ç§»é™¤éæ•¸å­—å­—å…ƒ
                except:
                    price = None

                # âœ… æŠ“å–ä¸»åœ–ç‰‡
                try:
                    main_img = driver.find_element(By.CSS_SELECTOR, 'div.clearfix a#cgo img#mainImg')
                    img_url = main_img.get_attribute("src")
                except:
                    img_url = None

                # âœ… æŠ“å–åˆ†é¡ (éºµåŒ…å±‘)
                try:
                    breadcrumb = driver.find_element(By.CSS_SELECTOR, 'li.breadcrumb-item.active')
                    category = breadcrumb.text.strip()
                except:
                    category = "æœªåˆ†é¡"



                product_dict={'name': title,
                            'price': price,
                            'img_url': img_url,
                            'product_url': product_url,
                            'classification': category,
                            'store':'SaveSafe',
                            }         
            except Exception as e:
                print(f" ç²å– {product_url} è©³æƒ…æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return product_dict
        # === è¨ªå•åˆ†é¡é é¢ ===
        def visit_link(url):
            """
            è¨ªå•ä¸€å€‹åˆ†é¡é é¢ï¼Œä¸¦æŠ“å–é é¢ä¸Šçš„æ‰€æœ‰ç”¢å“éˆæ¥ï¼Œç„¶å¾Œé€ä¸€é€²å…¥æ¯å€‹å•†å“é é¢é€²è¡Œè³‡æ–™çˆ¬å–ã€‚

            :param url: åˆ†é¡é é¢çš„ URL
            """
          
            driver = webdriver.Chrome(options=cls.chrome_options)
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.ThirdNavItemList.d-flex')))

                # âœ… æå–ç”¢å“éˆæ¥
                product_links = {p.get_attribute('href') for p in driver.find_elements(By.CSS_SELECTOR, 'div.card.hoverable a.text-center')}
                print(f"ğŸ” æ‰¾åˆ° {len(product_links)} å€‹ç”¢å“")

                # âœ… é€²å…¥æ¯å€‹ç”¢å“é é¢ï¼Œé‚Šçˆ¬é‚Šå¯«å…¥è³‡æ–™åº«
                for product_url in product_links:
                    get_product_details(driver, product_url)

            finally:
                driver.quit()

        # === ä¸»å‡½æ•¸ ===
        def savesafe_all_product():
            """
            ä¸»å‡½æ•¸ï¼Œè¨­ç½®è³‡æ–™åº«ä¸¦å•Ÿå‹•çˆ¬èŸ²ï¼ŒæŠ“å–åˆ†é¡é é¢çš„ç”¢å“éˆæ¥ï¼Œ
            ä¸¦ä½¿ç”¨å¤šç·šç¨‹ä¸¦è¡Œè™•ç†æ¯å€‹åˆ†é¡é é¢ã€‚
            """
            product_data = []
            

            driver = webdriver.Chrome(options=cls.chrome_options)
            try:
                driver.get("https://www.savesafe.com.tw/")
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.ThirdNavItemList.d-flex')))

                # âœ… æå–åˆ†é¡é é¢éˆæ¥
                linklist = {a.get_attribute('href') for a in driver.find_elements(By.CSS_SELECTOR, 'ul.ThirdNavItemList.d-flex li a')}
                print(f"ğŸŒ æ‰¾åˆ° {len(linklist)} å€‹åˆ†é¡é é¢")
            finally:
                driver.quit()

            # âœ… å¤šç·šç¨‹è™•ç†åˆ†é¡é é¢
            with ThreadPoolExecutor(max_workers=5) as executor:
                results = executor.map(visit_link, linklist)
                
                # æ”¶é›†æœ‰æ•ˆçš„å•†å“æ•¸æ“š
                product_data.extend(filter(None, results))
                
            return product_data
    @classmethod
    def poyabuy(cls):
        # å®£å‘Šåƒæ•¸
        url = "https://www.poyabuy.com.tw/v2/official/SalePageCategory/0?sortMode=Newest" 

        def get_category_index():
            """çˆ¬å–é¡åˆ¥åˆ—è¡¨ï¼Œä¸¦å­˜å…¥ `category` è³‡æ–™è¡¨ï¼ˆå¦‚æœåˆ†é¡ä¸å­˜åœ¨å‰‡æ–°å¢ï¼‰"""
            
            driver = webdriver.Chrome(options=cls.chrome_options)
            driver.get(url)
            driver.implicitly_wait(10)

            # å–å¾—æ‰€æœ‰åˆ†é¡æŒ‰éˆ•çš„åç¨±
            category_buttons = driver.find_elements(By.CLASS_NAME, "sc-kZCGdI.jmFZfR")
            category_names = []
                    
                
                
            for btn in tqdm(category_buttons[9:]):  # å¾ index=9 é–‹å§‹æŠ“å–
                category_names.append(btn.text)
                if btn.text == "ç”·å£«ä¿é¤Š":  # ç¢°åˆ° "ç”·å£«ä¿é¤Š" åœæ­¢
                    break

            driver.quit()  # é—œé–‰ç€è¦½å™¨

            print(f"ç™¼ç¾ {len(category_names)} å€‹åˆ†é¡: {category_names}")

            category_names_map = {}  # ç”¨ä¾†å­˜åˆ†é¡åç¨±å°æ‡‰çš„ ID
            for index,item in enumerate(category_names):
                category_names_map[index] = item
            
            return category_names_map  # å›å‚³åˆ†é¡ç´¢å¼•èˆ‡ ID å°æ‡‰è¡¨

        def fetch_category_url(index,category):
            
            """æ»¾å‹•åˆ°åº•éƒ¨ï¼Œçˆ¬å–å•†å“è³‡è¨Šï¼Œä¸¦å­˜å…¥ `product_detail` å’Œ `productl`"""
            
            driver = webdriver.Chrome(options=cls.chrome_options)
            wait = WebDriverWait(driver, 10, poll_frequency=0.5)
            driver.get(url)
            driver.implicitly_wait(10)
            sleep(3)
            # é»æ“Šåˆ†é¡æŒ‰éˆ•
            for _ in range(50):
                try:
                    category_buttons = driver.find_elements(By.CLASS_NAME, "sc-kZCGdI.jmFZfR")

                    category_buttons[index+9].click()
                    break
                except:
                    category_buttons[0].send_keys(Keys.ARROW_DOWN)
            try:        
                wait.until(EC.url_changes(driver.current_url))  # ç­‰å¾… URL è®Šæ›´
            except:
                pass
            # ç²å–åˆ†é¡åç¨±
            category_name = driver.find_element(By.CLASS_NAME, "sc-jdiFFc.ihCGgx").text
            print(f"é–‹å§‹çˆ¬å–åˆ†é¡: {category_name}")


            # æ»¾å‹•åˆ°æœ€åº•éƒ¨ï¼Œç¢ºä¿æ‰€æœ‰å•†å“éƒ½è¼‰å…¥
            last_height = driver.execute_script("return document.body.scrollHeight")
            while True:
                driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)
                sleep(2)  # ç­‰å¾…æ–°å…§å®¹è¼‰å…¥
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break  # å¦‚æœé é¢é«˜åº¦ä¸å†è®ŠåŒ–ï¼Œä»£è¡¨å·²ç¶“è¼‰å…¥å®Œç•¢
                last_height = new_height

            #  å–å¾—å®Œæ•´ HTMLï¼Œç„¶å¾Œç”¨ bs4 è§£æ
            soup = BeautifulSoup(driver.page_source, "html.parser")

            # çˆ¬å–å•†å“è³‡è¨Š
            products = soup.find_all("li", class_="column-grid-container__column")
            product_list = []
            
            for product in tqdm(products):
                try:
                    #  æª¢æŸ¥æ˜¯å¦åŒ…å« mask å±¤ï¼ˆå¦‚æœæœ‰å°±è·³éï¼‰
                    mask = product.find("div", class_="sc-hfiVbt hGbyjb")
                    if mask:
                        continue
                    
                    #  ä½¿ç”¨ `bs4` è§£æå•†å“è³‡è¨Š
                    product_name = product.find("div", class_="sc-bxgxFH xMesR").text.strip()
                    product_price = product.find("div", class_="sc-jEjhTi djfehP").text.replace("$", "").strip()
                    product_image_url = product.find("img")["src"]
                    product_url = product.find("a")["href"]
            
                    product_list.append((product_name, product_price, product_image_url, product_url))
                    product_dict={'name': product_name,
                            'price': product_price,
                            'img_url': product_image_url,
                            'product_url': product_url,
                            'classification': category,
                            'store':'carrefour',
                            }      
                except Exception as e:
                    print(f"çˆ¬å–å•†å“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    print( product_name, product_price, product_image_url, product_url)   
            driver.quit()
            return product_dict


        # å–å¾—æ‰€æœ‰åˆ†é¡ï¼Œå­˜å…¥ category è¡¨ï¼Œä¸¦ç²å–åˆ†é¡ ID å°æ‡‰è¡¨
        categorys = get_category_index()
        
        product_data=[]
        
    # âœ… å¤šç·šç¨‹è™•ç†åˆ†é¡é é¢
        with ThreadPoolExecutor(max_workers=5) as executor:

                results = executor.map(fetch_category_url, categorys.keys(), categorys.values())   
                product_data.extend(filter(None, results))
                
        return product_data

# ä½¿ç”¨æ–¹å¼Crawl.poyabuy()
